{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = \"<EOS>\"\n",
    "vocab = [\"what\", \"is\", \"statquest\", \"awesome\", eos]\n",
    "n_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ndim = 2\n",
    "\n",
    "        embedding_mean = torch.tensor(np.zeros((n_vocab, self.ndim)))\n",
    "        embedding_std = torch.tensor(np.ones((n_vocab, self.ndim)))\n",
    "        attention_mean = torch.tensor(np.zeros((self.ndim, self.ndim)))\n",
    "        attention_std = torch.tensor(np.ones((self.ndim, self.ndim)))\n",
    "\n",
    "        self.word_embedding_weights = nn.Parameter(torch.normal(mean=embedding_mean, std=embedding_std), requires_grad=True)\n",
    "        self.query_weights = nn.Parameter(torch.normal(mean=attention_mean, std=attention_std), requires_grad=True)\n",
    "        self.key_weights = nn.Parameter(torch.normal(mean=attention_mean, std=attention_std), requires_grad=True)\n",
    "        self.value_weights = nn.Parameter(torch.normal(mean=attention_mean, std=attention_std), requires_grad=True)\n",
    "\n",
    "    def word_embedding(self, inp):\n",
    "        return inp @ self.word_embedding_weights\n",
    "    \n",
    "    def positional_embedding(self, seq_len, n=10000):\n",
    "        P = torch.tensor(np.zeros((seq_len, self.ndim)))\n",
    "\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(self.ndim/2)):\n",
    "                denominator = np.power(n, 2*i/self.ndim)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "\n",
    "        return P\n",
    "    \n",
    "    def self_attention(self, inp):\n",
    "        querys = inp @ self.query_weights\n",
    "        keys = inp @ self.key_weights\n",
    "        querys @ torch.transpose(keys)\n",
    "        values = inp @ self.value_weights\n",
    "\n",
    "    def forward(self, inp):\n",
    "        word_embedding_res = self.word_embedding(inp)\n",
    "        pos_embed_res = self.positional_embedding(len(word_embedding_res), 10) + word_embedding_res\n",
    "        self_attention_res = self.self_attention(pos_embed_res)\n",
    "        return self_attention_res\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i)\n",
    "        loss = (output_i - label_i) ** 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1035, -0.1948],\n",
       "        [-0.3839, -0.9073],\n",
       "        [-0.3583,  0.2881],\n",
       "        [ 0.6836,  0.9758]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"what is statquest\"\n",
    "words_in_sentence = input_sentence.split(\" \") + [eos]\n",
    "one_hot_vectors = torch.tensor([[1. if vocab[i] == word else 0. for i in range(n_vocab)] for word in words_in_sentence], dtype=torch.float64)\n",
    "model(one_hot_vectors).detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
