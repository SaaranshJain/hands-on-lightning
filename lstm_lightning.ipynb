{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        \n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        \n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) + (input_value * self.wlr2) + self.blr1)\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) + (input_value * self.wpr2) + self.bpr1)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + (input_value * self.wp2) + self.bp1)\n",
    "        updated_long_memory = (long_memory * long_remember_percent) + (potential_remember_percent * potential_memory)\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) + (input_value * self.wo2) + self.bo1)\n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        return updated_long_memory, updated_short_memory\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        long_memory = 0\n",
    "        short_memory = 0\n",
    "\n",
    "        for i in range(len(inp)):\n",
    "            long_memory, short_memory = self.lstm_unit(inp[i], long_memory, short_memory)\n",
    "\n",
    "        return short_memory\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "        loss = (output_i - label_i) ** 2\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        if label_i == 0:\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now compare observed and predicted\n",
      "Company A: Observed = 0, Predicted = tensor(0.1243)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMbyHand()\n",
    "print(\"\\nNow compare observed and predicted\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company B: Observed = 1, Predicted = tensor(0.1441)\n"
     ]
    }
   ],
   "source": [
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-02-19 21:18:35.404398: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-19 21:18:35.503783: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-19 21:18:36.166956: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-19 21:18:36.167093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-19 21:18:36.291814: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-19 21:18:36.517654: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-19 21:18:36.522589: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-19 21:18:38.769925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 12    \n",
      "--------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/home/dacedev/Documents/hands-on-oreilly/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/dacedev/Documents/hands-on-oreilly/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1999: 100%|██████████| 2/2 [00:00<00:00, 70.74it/s, v_num=4] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1999: 100%|██████████| 2/2 [00:00<00:00, 57.35it/s, v_num=4]\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2000)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /home/dacedev/Documents/hands-on-oreilly/lightning_logs/version_0/checkpoints/epoch=1999-step=4000.ckpt\n",
      "/home/dacedev/Documents/hands-on-oreilly/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:360: The dirpath has changed from '/home/dacedev/Documents/hands-on-oreilly/lightning_logs/version_0/checkpoints' to '/home/dacedev/Documents/hands-on-oreilly/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 12    \n",
      "--------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /home/dacedev/Documents/hands-on-oreilly/lightning_logs/version_0/checkpoints/epoch=1999-step=4000.ckpt\n",
      "/home/dacedev/Documents/hands-on-oreilly/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/dacedev/Documents/hands-on-oreilly/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: 100%|██████████| 2/2 [00:00<00:00, 60.63it/s, v_num=1] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: 100%|██████████| 2/2 [00:00<00:00, 46.10it/s, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "trainer = L.Trainer(max_epochs=3000)\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningLSTM(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp_trans = inp.view(len(inp), 1)\n",
    "        lstm_out, temp = self.lstm(inp_trans)\n",
    "\n",
    "        prediction = lstm_out[-1]\n",
    "        return prediction\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "        loss = (output_i - label_i) ** 2\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        if label_i == 0:\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 128.12it/s, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 93.24it/s, v_num=3] \n"
     ]
    }
   ],
   "source": [
    "model = LightningLSTM()\n",
    "trainer = L.Trainer(max_epochs=200, log_every_n_steps=2)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
